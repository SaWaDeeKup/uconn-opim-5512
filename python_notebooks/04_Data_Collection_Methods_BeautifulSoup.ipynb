{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **#04: Data Collection Methods: Beautiful Soup**\n",
        "- Instructor: [Jaeung Sim](https://www.business.uconn.edu/person/jaeung-sim/) (University of Connecticut)\n",
        "- Course: OPIM 5512 Data Science Using Python\n",
        "- Last updated: February 6, 2025"
      ],
      "metadata": {
        "id": "WeVhR1jNVynW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objectives**\n",
        "1. Exercise web scraping and HTML parsing using `requests` and `Beautiful Soup`.\n",
        "2. Learn `Selenium` using Python (not Google Colab).\n",
        "\n",
        "**References**\n",
        "* [Tutorial: Web Scraping with Python Using Beautiful Soup](https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/)\n",
        "* [Beautiful Soup: Build a Web Scraper With Python](https://realpython.com/beautiful-soup-web-scraper-python/)"
      ],
      "metadata": {
        "id": "nKGZlOauV3G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Basic Information**\n",
        "\n",
        "**A. What is Web Scraping?**\n",
        "\n",
        ">**Web scraping** is the process of gathering information from the Internet. Even copying and pasting the lyrics of your favorite song is a form of web scraping! However, the words “web scraping” usually refer to a process that involves automation.\n",
        "\n",
        "**B. Reasons for Web Scraping**\n",
        "\n",
        ">Some websites offer data sets that are downloadable in CSV format, or accessible via an Application Programming Interface (API). But many websites with useful data don't offer these convenient options.\n",
        ">\n",
        ">If we wanted to analyze this data, or download it for use in some other app, we wouldn't want to painstakingly copy-paste everything. Web scraping is a technique that lets us use programming to do the heavy lifting.\n",
        ">\n",
        ">Automated web scraping can be a solution to speed up the data collection process. You write your code once, and it will get the information you want many times and from many pages.\n",
        "\n",
        "**C. Technical Challenges**\n",
        "\n",
        "* Variety and durability of websites\n",
        ">Every website is different. While you'll encounter general structures that repeat themselves, each website is unique and will need personal treatment if you want to extract the relevant information.\n",
        ">\n",
        ">Also, websites constantly change. Say you've built a shiny new web scraper that automatically cherry-picks what you want from your resource of interest. The first time you run your script, it works flawlessly. But when you run the same script only a short while later, you run into a discouraging and lengthy stack of tracebacks!\n",
        "\n",
        "* Hidden websites\n",
        "> Some pages contain information that's hidden behind a login. That means you'll need an account to be able to scrape anything from the page. The process to make an HTTP request from your Python script is different from how you access a page from your browser. Just because you can log in to the page through your browser doesn't mean you'll be able to scrape it with your Python script.\n",
        ">\n",
        "> Fortunately, some libraries come with their built-in capacity to handle authentication. With these techniques, you can log in to websites when making the HTTP request from your Python script and then scrape information that's hidden behind a login.\n",
        "\n",
        "* Dynamic websites\n",
        "> Unlike a static website, where the server sends you an HTML page that already contains all the page information in the response, with a dynamic website, the server might not send back any HTML at all. Instead, you could receive JavaScript code as a response, which will look completely different from what you saw when you inspected the page with your browser's developer tools. The only way to go from the JavaScript code you received to the content that you're interested in is to execute the code, just like your browser does.\n",
        ">\n",
        ">There are some solutions for this. For example, `requests-html` is a project created by the author of the `requests` library that allows you to render JavaScript using syntax that's similar to the syntax in `requests`. It also includes capabilities for parsing the data by using `Beautiful Soup` under the hood. Another popular choice for scraping dynamic content is `Selenium`. You can think of Selenium as a slimmed-down browser that executes the JavaScript code for you before passing on the rendered HTML response to your script.\n",
        "\n",
        "**D. Legal Challenges**\n",
        ">Unfortunately, there's not a cut-and-dry answer on whether web scraping is legal. Some websites explicitly allow web scraping. Others explicitly forbid it. Many websites don't offer any clear guidance one way or the other.\n",
        ">\n",
        ">Before scraping any website, we should look for a terms and conditions page to see if there are explicit rules about scraping. If there are, we should follow them. If there are not, then it becomes more of a judgement call.\n",
        ">\n",
        ">If you're scraping a page respectfully for educational purposes, then you're usually okay. Still, it's a good idea to do some research on your own and make sure that you're not violating any terms of service before you start a large-scale project.\n",
        ">\n",
        ">Remember, though, that web scraping consumes server resources for the host website. If we're just scraping one page once, that isn't going to cause a problem. But if our code is scraping 1,000 pages once every ten minutes, that could quickly get expensive for the website owner.\n",
        ">\n",
        ">Thus, in addition to following any and all explicit rules about web scraping posted on the site, it's also a good idea to follow these best practices:\n",
        ">* Never scrape more frequently than you need to.\n",
        ">* Consider caching the content you scrape so that it's only downloaded once.\n",
        ">* Build pauses into your code using functions like `time.sleep()` to keep from overwhelming servers with too many requests too quickly.\n",
        "\n",
        "**E. Alternatives**\n",
        "* Alternative web scrappers\n",
        "> There are several free web crawlers you can use without complex programming. For instance, Google Chrome provides its own web scraper as an extension tool.\n",
        "\n",
        "* Application Programing Interfaces (APIs)\n",
        ">Some website providers offer application programming interfaces (APIs) that allow you to access their data in a predefined manner. With APIs, you can avoid parsing HTML. Instead, you can access the data directly using formats like JSON and XML. HTML is primarily a way to present content to users visually.\n",
        ">\n",
        ">When you use an API, the process is generally more stable than gathering the data through web scraping. That’s because developers create APIs to be consumed by programs rather than by human eyes.\n",
        "\n",
        "**References:**\n",
        "* [Beautiful Soup: Build a Web Scraper With Python](https://realpython.com/beautiful-soup-web-scraper-python/)\n",
        "* [Tutorial: Web Scraping with Python Using Beautiful Soup](https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/)"
      ],
      "metadata": {
        "id": "Uit_efpHtMFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 0. Basic Setup**"
      ],
      "metadata": {
        "id": "p5AzdL0ymjpM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYXdoIz_VsRR"
      },
      "outputs": [],
      "source": [
        "# Run this code to import the NumPy and Pandas modules\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 1. `requests` and `Beautiful Soup`**"
      ],
      "metadata": {
        "id": "ZuEtRVrcmscY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import `request` library\n",
        "# !pip install requests # \"Requirement already satisfied\"\n",
        "import requests"
      ],
      "metadata": {
        "id": "Rwdrlt0Bm795"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import `BeautifulSoup` function\n",
        "# !pip install beautifulsoup4 # \"Requirement already satisfied\"\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "hpWBn3TBC9oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Scraping the Fake Python Job Site**\n",
        "\n",
        "Objectives\n",
        "* Step through a web scraping pipeline from start to finish.\n",
        "* Inspect the HTML structure of your target site with your browser’s developer tools.\n",
        "* Decipher the data encoded in URLs.\n",
        "* Download the page’s HTML content using Python’s `requests` library.\n",
        "* Parse the downloaded HTML with `Beautiful Soup` to extract relevant information.\n",
        "* Convert the information into a DataFrame using `Pandas`.\n",
        "\n",
        "Data source: https://realpython.github.io/fake-jobs/"
      ],
      "metadata": {
        "id": "u3JuImvuyK4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1. Inspecting the Data Source**\n",
        "\n",
        "1.1. Exploring the website\n",
        "\n",
        ">Click through the site and interact with it just like any typical job searcher would. For example, you can scroll through the main page of the website.\n",
        "\n",
        "1.2. Deciphering the information in URLs\n",
        "\n",
        ">A programmer can encode a lot of information in a URL. Your web scraping journey will be much easier if you first become familiar with how URLs work and what they’re made of. For example, you might find yourself on a details page that has the following URL:\n",
        ">\n",
        ">>https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\n",
        ">\n",
        ">You can deconstruct the above URL into two main parts:\n",
        ">1. **The base URL** represents the path to the search functionality of the website. In the example above, the base URL is `https://realpython.github.io/fake-jobs/`.\n",
        ">1. **The specific site location** that ends with `.html` is the path to the job description's unique resource.\n",
        ">\n",
        ">Any job posted on this website will use the same base URL. However, the unique resources’ location will be different depending on what specific job posting you’re viewing.\n",
        "\n",
        "1.3. Inspecting the site using developer tools\n",
        "\n",
        ">**Developer tools** can help you understand the structure of a website. All modern browsers come with developer tools installed. In this section, you’ll see how to work with the developer tools in Chrome. The process will be very similar to other modern browsers.\n",
        ">* **Chrome on Mac:** (*menu*) View → Developer → Developer Tools, (*keyboard shortcut*) Cmd + Alt + I\n",
        ">* **Chrome on Windows/Linux:** (*menu*) The top-right menu button (⋮) → More Tools → Developer Tools, (*keyboard shortcut*) Ctrl + Shift + I\n",
        ">\n",
        ">Developer tools allow you to interactively explore the site’s document object model (DOM) to better understand your source. To dig into your page’s DOM, select the Elements tab in developer tools. You’ll see a structure with clickable HTML elements. You can expand, collapse, and even edit elements right in your browser."
      ],
      "metadata": {
        "id": "UMHS55IE_1t7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2. Scraping HTML Content from a Page with `requests`**"
      ],
      "metadata": {
        "id": "kLm9XuTiBT2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://realpython.github.io/fake-jobs/\"\n",
        "page = requests.get(URL) # HTTP `get` request to get or retrieve data\n",
        "\n",
        "print(page.text) # Show the inspected HTML from 'page.text' object"
      ],
      "metadata": {
        "id": "GyFbJTQlDv-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are interested in the following classes:\n",
        "* `class=\"title is-5\"` contains the title of the job posting.\n",
        "* `class=\"subtitle is-6 company\"` contains the name of the company that offers the position.\n",
        "* `class=\"location\"` contains the location where you would be working.\n",
        "\n"
      ],
      "metadata": {
        "id": "hF9EOw7TGglU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3. Parsing HTML Code with `Beautiful Soup`**"
      ],
      "metadata": {
        "id": "Eb__YLXvBfVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Beautiful Soup object\n",
        "soup = BeautifulSoup(page.content, \"html.parser\") # Taking 'page.content' instead of 'page.text'"
      ],
      "metadata": {
        "id": "MyNNEOYCBfCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1. Find elements by ID\n",
        "> In an HTML web page, every element can have an `id` attribute assigned. As the name already suggests, that `id` attribute makes the element uniquely identifiable on the page.\n",
        ">\n",
        "> From the current HTML page, you can find a `<div>` with an `id` attribute that has the value `\"ResultsContainer\"` as shown below:\n",
        ">```html\n",
        "><div id=\"ResultsContainer\" class=\"columns is-multiline\">\n",
        ">```"
      ],
      "metadata": {
        "id": "aa6ajLzjMwjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the specific HTML element by its ID\n",
        "results = soup.find(id=\"ResultsContainer\")"
      ],
      "metadata": {
        "id": "vI09LrQEMw8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show all the HTML contained within the <div>\n",
        "print(results.prettify())"
      ],
      "metadata": {
        "id": "-sIIlPeoOgbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2. Find elements by HTML class name"
      ],
      "metadata": {
        "id": "dvh2U-1kND2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all job-posting elements\n",
        "job_elements = results.find_all(\"div\", class_=\"card-content\") # Wrapped in a <div> element with the class 'card-content'"
      ],
      "metadata": {
        "id": "ZOy0CoxGNEHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at all the job postings\n",
        "for job_element in job_elements:\n",
        "  print(job_element)\n",
        "  print(\"--------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "T9Y3UVHpQiex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick out child elements with descriptive class names from each job posting\n",
        "for job_element in job_elements:\n",
        "  title_element = job_element.find(\"h2\", class_=\"title is-5\")\n",
        "  company_element = job_element.find(\"h3\", class_=\"subtitle is-6 company\")\n",
        "  location_element = job_element.find(\"p\", class_=\"location\")\n",
        "  print(title_element)\n",
        "  print(company_element)\n",
        "  print(location_element)\n",
        "  print()"
      ],
      "metadata": {
        "id": "-EMHPKCTRwoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3. Extract text from HTML elements"
      ],
      "metadata": {
        "id": "62EdK-0GTp4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# .text to return the text content only\n",
        "for job_element in job_elements:\n",
        "  title_element = job_element.find(\"h2\", class_=\"title is-5\")\n",
        "  company_element = job_element.find(\"h3\", class_=\"subtitle is-6 company\")\n",
        "  location_element = job_element.find(\"p\", class_=\"location\")\n",
        "  print(title_element.text)\n",
        "  print(company_element.text)\n",
        "  print(location_element.text)\n",
        "  print()"
      ],
      "metadata": {
        "id": "r-_WBjHxTqPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# .text.strip() to remove extra whitespace\n",
        "for job_element in job_elements:\n",
        "  title_element = job_element.find(\"h2\", class_=\"title is-5\")\n",
        "  company_element = job_element.find(\"h3\", class_=\"subtitle is-6 company\")\n",
        "  location_element = job_element.find(\"p\", class_=\"location\")\n",
        "  print(title_element.text.strip())\n",
        "  print(company_element.text.strip())\n",
        "  print(location_element.text.strip())\n",
        "  print()"
      ],
      "metadata": {
        "id": "MV1Gnp0zUCQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4. Constructing a DataFrame from the HTML"
      ],
      "metadata": {
        "id": "0U5WwBNBcqji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a DataFrame with empty columns\n",
        "job_postings = pd.DataFrame(columns=[\"title\", \"company\", \"location\"])"
      ],
      "metadata": {
        "id": "vGjun0mKcqA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a DataFrame from 'job_elements'\n",
        "for job_element in job_elements:\n",
        "    title_element = job_element.find(\"h2\", class_=\"title is-5\")\n",
        "    company_element = job_element.find(\"h3\", class_=\"subtitle is-6 company\")\n",
        "    location_element = job_element.find(\"p\", class_=\"location\")\n",
        "\n",
        "    # Create a temporary DataFrame for the current job posting\n",
        "    temp_df = pd.DataFrame([[title_element, company_element, location_element]], columns=[\"title\", \"company\", \"location\"])\n",
        "\n",
        "    # Use concat to add the new row to the DataFrame\n",
        "    job_postings = pd.concat([job_postings, temp_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "DHR7sowdgLDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "job_postings.head()"
      ],
      "metadata": {
        "id": "zdMdbolRfgb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revise the code to make each block of the DataFrame clean texts."
      ],
      "metadata": {
        "id": "zblaZauPf_Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a DataFrame with empty columns\n",
        "job_postings = pd.DataFrame(columns=[\"title\", \"company\", \"location\"])"
      ],
      "metadata": {
        "id": "CcHxxXHYhWYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Revise the Construct a DataFrame from 'job_elements'\n",
        "for job_element in job_elements:\n",
        "    title_element = job_element.find(\"h2\", class_=\"title is-5\")\n",
        "    company_element = job_element.find(\"h3\", class_=\"subtitle is-6 company\")\n",
        "    location_element = job_element.find(\"p\", class_=\"location\")\n",
        "    temp_df = pd.DataFrame([[title_element.text.strip(), company_element.text.strip(), location_element.text.strip()]], columns=[\"title\", \"company\", \"location\"]) # Updating this line\n",
        "    job_postings = pd.concat([job_postings, temp_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "ULgUF9IwgEr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "job_postings.head()"
      ],
      "metadata": {
        "id": "VDi0_m_qgFmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. Scraping Actual Weather Data**\n",
        "\n",
        "Objectives\n",
        "* Download the web page containing the weather forecast using `requests`.\n",
        "* Create a `BeautifulSoup` class to parse the page.\n",
        "* Extract and print the extended forecast for San Francisco.\n",
        "* Convert the information into a DataFrame using `Pandas`.\n",
        "\n",
        "Data source: https://forecast.weather.gov/MapClick.php?lon=-73.53080749511717&lat=41.053272202459226"
      ],
      "metadata": {
        "id": "WiKeMD5xA-tV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1. Inspecting the Data Source**\n",
        "\n",
        "1.1. Exploring the website\n",
        "\n",
        "1.2. Deciphering the information in URLs\n",
        "\n",
        "1.3. Inspecting the site using developer tools"
      ],
      "metadata": {
        "id": "bzGrkkDGTMuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2. Scraping HTML Content from a Page with `requests`**"
      ],
      "metadata": {
        "id": "aSDZlozOTM9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page = requests.get(\"https://forecast.weather.gov/MapClick.php?lon=-73.53080749511717&lat=41.053272202459226\") # HTTP `get` request to get or retrieve data\n",
        "print(page.text) # Show the inspected HTML from 'page.text' object"
      ],
      "metadata": {
        "id": "x03dFAokmuGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3. Parsing HTML Code with `Beautiful Soup`**"
      ],
      "metadata": {
        "id": "Z5zyfAU8U0Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Beautiful Soup object\n",
        "soup = BeautifulSoup(page.content, \"html.parser\") # Taking 'page.content' instead of 'page.text'"
      ],
      "metadata": {
        "id": "xO03ykQVU0eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1. Find elements by ID"
      ],
      "metadata": {
        "id": "fT9owxVQ_SGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the specific HTML element by its ID\n",
        "results = soup.find(id=\"seven-day-forecast-container\")"
      ],
      "metadata": {
        "id": "g94nWawA-Ypy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show all the HTML contained within the <div>\n",
        "print(results.prettify())"
      ],
      "metadata": {
        "id": "IfeCcVdSAD0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2. Find elements by HTML class name"
      ],
      "metadata": {
        "id": "vu1-EHSLAjVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all forecast elements\n",
        "forecast_items = results.find_all(\"div\", class_=\"tombstone-container\")"
      ],
      "metadata": {
        "id": "ld2NFm23Ajp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at all forecast items\n",
        "for forecast_item in forecast_items:\n",
        "  print(forecast_item)\n",
        "  print(\"--------------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "UC8WpWdLBYYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick out child elements with descriptive class names from each job posting\n",
        "for forecast_item in forecast_items:\n",
        "  period_element = forecast_item.find(\"p\", class_=\"period-name\")\n",
        "  cloud_element = forecast_item.find(\"p\", class_=\"short-desc\")\n",
        "  temperature_element = forecast_item.find(\"p\", class_=\"temp\")\n",
        "  print(period_element)\n",
        "  print(cloud_element)\n",
        "  print(temperature_element)\n",
        "  print()"
      ],
      "metadata": {
        "id": "tCRarYDRCO95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revise the code to 1) extract text from HTML elements and 2) make each block of the DataFrame clean texts."
      ],
      "metadata": {
        "id": "yq7HmThLCOf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Dataframe with empty columns\n",
        "forecasts = pd.DataFrame(columns=[\"period\", \"cloud\", \"temperature\"])"
      ],
      "metadata": {
        "id": "qsmumeLiDuq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a DataFrame from 'forecast_items'\n",
        "for forecast_item in forecast_items:\n",
        "  period_element = forecast_item.find(\"p\", class_=\"period-name\")\n",
        "  cloud_element = forecast_item.find(\"p\", class_=\"short-desc\")\n",
        "  temperature_element = forecast_item.find(\"p\", class_=\"temp\")\n",
        "  temp_df = pd.DataFrame([[period_element.text.strip(), cloud_element.text.strip(), temperature_element.text.strip()]], columns=[\"period\", \"cloud\", \"temperature\"])\n",
        "  forecasts = pd.concat([forecasts, temp_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "ts9_nK1WD_K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "forecasts.head(10)"
      ],
      "metadata": {
        "id": "n6Swx6H8FCLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References:**\n",
        "* [Beautiful Soup: Build a Web Scraper With Python](https://realpython.com/beautiful-soup-web-scraper-python/)\n",
        "* [Tutorial: Web Scraping with Python Using Beautiful Soup](https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/)"
      ],
      "metadata": {
        "id": "Kjk0DqDSuLiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 2. `Selenium`**\n",
        "\n",
        "Please refer to the other iPython notebook (`\"05_Data_Collection_Methods_Selenium.ipynb\"`)."
      ],
      "metadata": {
        "id": "wy2SjQWKoE5t"
      }
    }
  ]
}